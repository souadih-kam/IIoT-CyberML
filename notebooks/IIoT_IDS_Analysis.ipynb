{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Unified Intrusion Detection for Industrial IoT (Text-Only Stats Version)\n## Hybrid Machine Learning Framework\n\nThis notebook implements a comprehensive framework for IIoT Intrusion Detection, harmonizing four major datasets:\n- X-IIoTID\n- Edge-IIoTset\n- WUSTL-IIOT-2021\n- TON-IoT\n\n**Note:** This version is configured to output statistics to the console/notebook output only, without generating files or plots.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport logging\nimport json\nimport time\nimport os\nimport platform\nimport psutil\nimport threading\nimport gc\nfrom datetime import datetime\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (classification_report, confusion_matrix, accuracy_score,\n                             precision_score, roc_curve, auc, roc_auc_score, \n                             recall_score, f1_score)\nfrom sklearn.utils import shuffle\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.backends.backend_pdf import PdfPages\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import ML models\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n\n# Logging configuration\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n\n# Global variables for resource monitoring\nresource_logs = []\nmonitoring_active = False\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n# RESOURCE MONITORING FUNCTIONS\n# ============================================\n\ndef get_gpu_info():\n    \"\"\"Get GPU information using nvidia-smi if available\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total', \n                               '--format=csv,noheader,nounits'], \n                              capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            lines = result.stdout.strip().split('\\n')\n            gpu_info = []\n            for line in lines:\n                parts = line.split(',')\n                if len(parts) >= 2:\n                    gpu_info.append({'name': parts[0].strip(), 'memory_mb': float(parts[1].strip())})\n            return gpu_info\n    except:\n        pass\n    return None\n\ndef get_gpu_usage():\n    \"\"\"Get current GPU memory usage and utilization\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used,utilization.gpu', \n                               '--format=csv,noheader,nounits'], \n                              capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            lines = result.stdout.strip().split('\\n')\n            gpu_stats = []\n            for line in lines:\n                parts = line.split(',')\n                if len(parts) >= 2:\n                    gpu_stats.append({\n                        'memory_used_mb': float(parts[0].strip()),\n                        'utilization_percent': float(parts[1].strip())\n                    })\n            return gpu_stats[0] if gpu_stats else {'memory_used_mb': 0, 'utilization_percent': 0}\n    except:\n        pass\n    return {'memory_used_mb': 0, 'utilization_percent': 0}\n\ndef monitor_resources(interval=2.0):\n    \"\"\"Background thread to monitor system resources\"\"\"\n    global resource_logs, monitoring_active\n    \n    process = psutil.Process()\n    \n    while monitoring_active:\n        try:\n            cpu_percent = psutil.cpu_percent(interval=0.1)\n            ram_info = psutil.virtual_memory()\n            ram_used_mb = ram_info.used / (1024**2)\n            \n            disk_io = psutil.disk_io_counters()\n            disk_read_mb = disk_io.read_bytes / (1024**2)\n            disk_write_mb = disk_io.write_bytes / (1024**2)\n            \n            gpu_stats = get_gpu_usage()\n            \n            log_entry = {\n                'timestamp': datetime.now().isoformat(),\n                'cpu_percent': cpu_percent,\n                'ram_used_mb': ram_used_mb,\n                'disk_read_mb': disk_read_mb,\n                'disk_write_mb': disk_write_mb,\n                'gpu_mem_used_mb': gpu_stats['memory_used_mb'],\n                'gpu_util_percent': gpu_stats['utilization_percent']\n            }\n            \n            resource_logs.append(log_entry)\n            time.sleep(interval)\n            \n        except Exception as e:\n            logging.error(f\"Error monitoring resources: {e}\")\n            break\n\ndef start_monitoring():\n    \"\"\"Start resource monitoring in background thread\"\"\"\n    global monitoring_active, resource_logs\n    monitoring_active = True\n    resource_logs = []\n    monitor_thread = threading.Thread(target=monitor_resources, daemon=True)\n    monitor_thread.start()\n    return monitor_thread\n\ndef stop_monitoring():\n    \"\"\"Stop resource monitoring\"\"\"\n    global monitoring_active\n    monitoring_active = False\n    time.sleep(2.5)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_environment_metadata(seed=42, batch_size=32):\n    \"\"\"Collect comprehensive environment metadata\"\"\"\n    metadata = {\n        'date': datetime.now().isoformat(),\n        'machine_name': platform.node(),\n        'os': f\"{platform.system()} {platform.release()}\",\n        'cpu': {\n            'model': platform.processor(),\n            'physical_cores': psutil.cpu_count(logical=False),\n            'logical_cores': psutil.cpu_count(logical=True),\n        },\n        'ram_total_gb': psutil.virtual_memory().total / (1024**3),\n        'seed': seed,\n        'batch_size': batch_size\n    }\n    \n    gpu_info = get_gpu_info()\n    if gpu_info:\n        metadata['gpu'] = gpu_info\n    else:\n        metadata['gpu'] = 'Not available'\n    \n    import sys\n    import sklearn\n    metadata['versions'] = {\n        'python': sys.version,\n        'numpy': np.__version__,\n        'pandas': pd.__version__,\n        'scikit-learn': sklearn.__version__,\n    }\n    \n    try:\n        import xgboost\n        metadata['versions']['xgboost'] = xgboost.__version__\n    except:\n        pass\n    \n    return metadata\n\ndef measure_inference_latency(model, X_sample, n_runs=5, batch_sizes=[1, 32], device='CPU'):\n    \"\"\"Measure inference latency and throughput\"\"\"\n    results = []\n    \n    for batch_size in batch_sizes:\n        latencies = []\n        ram_usage = []\n        gpu_usage = []\n        \n        # Warm-up\n        for _ in range(2):\n            if batch_size == 1:\n                _ = model.predict(X_sample[:1])\n            else:\n                _ = model.predict(X_sample[:min(batch_size, len(X_sample))])\n        \n        # Actual measurements\n        for run in range(n_runs):\n            ram_before = psutil.virtual_memory().used / (1024**2)\n            gpu_stats_before = get_gpu_usage()\n            \n            start_time = time.perf_counter()\n            if batch_size == 1:\n                _ = model.predict(X_sample[:1])\n            else:\n                _ = model.predict(X_sample[:min(batch_size, len(X_sample))])\n            end_time = time.perf_counter()\n            \n            latency_ms = (end_time - start_time) * 1000\n            latencies.append(latency_ms)\n            \n            ram_after = psutil.virtual_memory().used / (1024**2)\n            gpu_stats_after = get_gpu_usage()\n            \n            ram_usage.append(ram_after - ram_before)\n            gpu_usage.append(gpu_stats_after['memory_used_mb'] - gpu_stats_before['memory_used_mb'])\n        \n        latencies_np = np.array(latencies)\n        throughput = (batch_size * 1000) / np.mean(latencies)\n        \n        result = {\n            'device': device,\n            'batch_size': batch_size,\n            'latency_mean_ms': float(np.mean(latencies)),\n            'latency_p50_ms': float(np.percentile(latencies, 50)),\n            'latency_p95_ms': float(np.percentile(latencies, 95)),\n            'latency_max_ms': float(np.max(latencies)),\n            'throughput_samples_per_s': float(throughput),\n            'ram_used_mb_during_inference': float(np.mean(ram_usage)) if ram_usage else 0,\n            'gpu_mem_used_mb_during_inference': float(np.mean(gpu_usage)) if gpu_usage else 0\n        }\n        \n        results.append(result)\n        logging.info(f\"Latency - Device: {device}, Batch: {batch_size}, Mean: {result['latency_mean_ms']:.2f}ms\")\n    \n    return results\n\n# Color palette definition for visualization\nn_colors = 20\ncolors = plt.cm.get_cmap('viridis', n_colors)\ncmap = ListedColormap(colors(np.linspace(0, 1, n_colors)))\ndataset_colors = {1: '\\033[91m', 2: '\\033[94m', 3: '\\033[92m', 4: '\\033[93m', 'reset': '\\033[0m'}\ndataset_plot_colors = {1: 'red', 2: 'blue', 3: 'green', 4: 'orange'}\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n# Data preparation functions\n# ---------------------------\ndef prepare_data_1(path_csv, nrows=100000):\n    logging.info(\"Loading and preparing data from Dataset1...\")\n    data = pd.read_csv(path_csv, nrows=nrows)\n    data = pd.get_dummies(data, columns=['Protocol', 'Service'])\n    y = data['class1']\n    data = data.drop(['Date', 'Timestamp', 'Scr_IP', 'Des_IP', 'class1', 'class2', 'class3'], axis=1)\n    data = data.replace({False: 0, 'FALSE': 0, 'false': 0, True: 1, 'TRUE': 1, 'true': 1,\n                         '-': np.nan, '?': np.nan, '': np.nan, ' ': np.nan}).replace({'[A-Za-z]': np.nan}, regex=True)\n    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n    data = data.dropna(thresh=int(0.7 * len(data)), axis=1)\n    numeric_cols = data.select_dtypes(include=[np.number]).columns\n    data[numeric_cols] = data[numeric_cols].fillna(data[numeric_cols].median())\n    le = LabelEncoder()\n    y_int = le.fit_transform(y)\n    class_labels1 = dict(zip(y_int, y))\n    class_dataset_map1 = {label: 1 for label in y.unique()}\n    data = data.astype('float32')\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(data)\n    df_scaled = pd.DataFrame(scaled, columns=data.columns)\n    return df_scaled, y_int, le, scaler, class_labels1, class_dataset_map1\n\ndef remove_outliers(df, threshold=1.5):\n    if 'Attack_type' in df.columns:\n        groups = []\n        for cl in df['Attack_type'].unique():\n            group = df[df['Attack_type'] == cl]\n            numeric_cols = group.select_dtypes(include=[np.number]).columns\n            for col in numeric_cols:\n                Q1 = group[col].quantile(0.25)\n                Q3 = group[col].quantile(0.75)\n                IQR = Q3 - Q1\n                group = group[(group[col] >= Q1 - threshold * IQR) & (group[col] <= Q3 + threshold * IQR)]\n            groups.append(group)\n        return pd.concat(groups)\n    return df\n\ndef add_rolling_features(df, window=3):\n    numeric_cols = df.select_dtypes(include=[np.number]).columns\n    for col in numeric_cols:\n        df[f'{col}_roll_mean'] = df[col].rolling(window=window, min_periods=1).mean()\n        df[f'{col}_roll_std'] = df[col].rolling(window=window, min_periods=1).std().fillna(0)\n    return df\n\ndef prepare_data_2(path_csv):\n    logging.info(\"Loading and preparing data from Dataset2...\")\n    df = pd.read_csv(path_csv, low_memory=False)\n    cols_to_drop = [\"frame.time\", \"ip.src_host\", \"ip.dst_host\", \"arp.src.proto_ipv4\",\n                    \"arp.dst.proto_ipv4\", \"http.file_data\", \"http.request.full_uri\",\n                    \"icmp.transmit_timestamp\", \"http.request.uri.query\", \"tcp.options\",\n                    \"tcp.payload\", \"tcp.srcport\", \"tcp.dstport\", \"udp.port\", \"mqtt.msg\"]\n    df = df.drop(columns=cols_to_drop, errors='ignore').dropna().drop_duplicates()\n    df = remove_outliers(df, threshold=1.5)\n    df = add_rolling_features(df, window=5)\n    categorical_cols = ['http.request.method', 'http.referer', 'http.request.version',\n                        'dns.qry.name.len', 'mqtt.conack.flags', 'mqtt.protoname', 'mqtt.topic']\n    for col in categorical_cols:\n        if col in df.columns:\n            df = pd.get_dummies(df, columns=[col])\n    le = LabelEncoder()\n    y = le.fit_transform(df['Attack_type'])\n    class_labels2 = dict(zip(y, df['Attack_type']))\n    class_dataset_map2 = {label: 2 for label in df['Attack_type'].unique()}\n    X_df = df.drop(columns=['Attack_type'])\n    X_df = X_df.astype('float32')\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(X_df)\n    df_scaled = pd.DataFrame(scaled, columns=X_df.columns)\n    return df_scaled, y, scaler, np.unique(df['Attack_type']), class_labels2, class_dataset_map2\n\ndef prepare_data_3(path_csv):\n    logging.info(\"Loading and preparing data from Dataset3...\")\n    data = pd.read_csv(path_csv)\n    remove_cols = ['StartTime', 'LastTime', 'SrcAddr', 'DstAddr', 'sIpId', 'dIpId']\n    data = data.drop(remove_cols + ['Target'], axis=1)\n    data.drop_duplicates(inplace=True)\n    y = data['Traffic']\n    data = data.drop(columns=['Traffic'])\n    logging.info(\"Selecting numeric columns in Dataset3...\")\n    data = data.select_dtypes(include=[np.number])\n    logging.info(\"Filling missing values with median in Dataset3...\")\n    data.fillna(data.median(), inplace=True)\n    le = LabelEncoder()\n    y_int = le.fit_transform(y)\n    class_labels3 = dict(zip(y_int, y))\n    class_dataset_map3 = {label: 3 for label in y.unique()}\n    data = data.astype('float32')\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(data)\n    df_scaled = pd.DataFrame(scaled, columns=data.columns)\n    return df_scaled, y_int, le, scaler, class_labels3, class_dataset_map3\n\ndef prepare_data_4(path_csv):\n    logging.info(\"Loading and preparing data from Dataset4...\")\n    data = pd.read_csv(path_csv)\n    data = data.drop(columns=[\"ts\", \"src_ip\", \"src_port\", \"dst_port\", \"dst_ip\"])\n    data = data.replace(\"-\", np.NaN).apply(lambda x: x.fillna(x.value_counts().index[0]))\n    data.drop_duplicates(inplace=True)\n    y = data['type']\n    data = data.drop(columns=['type', 'label'])\n    logging.info(\"Selecting numeric columns in Dataset4...\")\n    data = data.select_dtypes(include=[np.number])\n    logging.info(\"Filling missing values with median in Dataset4...\")\n    data.fillna(data.median(), inplace=True)\n    le = LabelEncoder()\n    y_int = le.fit_transform(y)\n    class_labels4 = dict(zip(y_int, y))\n    class_dataset_map4 = {label: 4 for label in y.unique()}\n    data = data.astype('float32')\n    scaler = StandardScaler()\n    scaled = scaler.fit_transform(data)\n    df_scaled = pd.DataFrame(scaled, columns=data.columns)\n    return df_scaled, y_int, le, scaler, class_labels4, class_dataset_map4\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---------------------------\n# VISUALIZATION FUNCTIONS\n# ---------------------------\n    print(f\"Stats for {dataset_name}: Total Samples: {total_samples}\")\n    return None\n\n\n    print(\"Combined Dataset stats calculated.\")\n    return None\n\n\n    print(\"Dataset comparisons calculated.\")\n    return None\n\n\n# ============================================\n# HIGH-QUALITY CONFUSION MATRIX FUNCTIONS\n# ============================================\n\n    print(f\"Confusion Matrix for {title} calculated (Text only).\")\n    return None\n\n\n    print(f\"Compact Confusion Matrix for {title} calculated (Text only).\")\n    return None\n\n\n    return None\n\n    return None\n\ndef identify_minority_classes(y, threshold_percentage=0.01):\n    class_counts = pd.Series(y).value_counts()\n    total_samples = len(y)\n    minority_classes = class_counts[class_counts / total_samples < threshold_percentage].index\n    return minority_classes\n\ndef colored_classification_report(report, class_dataset_mapping, all_classes_labels_encoded_to_name):\n    lines = report.split('\\n')\n    colored_lines = []\n    for line in lines:\n        if not line.strip():\n            colored_lines.append(line)\n            continue\n        parts = line.split()\n        if not parts:\n            colored_lines.append(line)\n            continue\n        try:\n            class_name_candidate = parts[0]\n            encoded_value = None\n            for enc, name in all_classes_labels_encoded_to_name.items():\n                if name == class_name_candidate:\n                    encoded_value = enc\n                    break\n            if encoded_value is None and len(parts) > 1:\n                class_name_candidate = parts[1]\n                for enc, name in all_classes_labels_encoded_to_name.items():\n                    if name == class_name_candidate:\n                        encoded_value = enc\n                        break\n            if encoded_value is not None:\n                original_class_name = all_classes_labels_encoded_to_name[encoded_value]\n                dataset_id = None\n                for name, ds_id in class_dataset_mapping.items():\n                    if name == original_class_name:\n                        dataset_id = ds_id\n                        break\n                if dataset_id:\n                    colored_class_name = f\"{dataset_colors[dataset_id]}{original_class_name}{dataset_colors['reset']}\"\n                    colored_line = line.replace(original_class_name, colored_class_name, 1)\n                    colored_lines.append(colored_line)\n                else:\n                    colored_lines.append(line)\n            else:\n                colored_lines.append(line)\n        except Exception as e:\n            colored_lines.append(line)\n    return '\\n'.join(colored_lines)\n\n    print(\"Summary table calculated.\")\n    return None\n\n\ndef generate_dataset_statistics_latex(output_dir, df1, df2, df3, df4, \n                                      y1, y2, y3, y4,\n                                      labels1, labels2, labels3, labels4,\n                                      all_columns):\n    \"\"\"\n    Generate a publication-quality LaTeX table for dataset statistics\n    \"\"\"\n    \n    # Calculate statistics for each dataset\n    dataset_stats = [\n        {\n            'name': 'X-IIoTID',\n            'citation': '\\\\cite{xiiotid2022}',\n            'samples': len(y1),\n            'original_features': len(df1.columns),\n            'attack_examples': sorted(list(set(labels1.values())))[:5]\n        },\n        {\n            'name': 'Edge-IIoTset',\n            'citation': '\\\\cite{ferrag2022edge}',\n            'samples': len(y2),\n            'original_features': len(df2.columns),\n            'attack_examples': sorted(list(set(labels2.values())))[:5]\n        },\n        {\n            'name': 'WUSTL-IIOT-2021',\n            'citation': '\\\\cite{zolanvari2021wustl}',\n            'samples': len(y3),\n            'original_features': len(df3.columns),\n            'attack_examples': sorted(list(set(labels3.values())))[:5]\n        },\n        {\n            'name': 'TON-IoT',\n            'citation': '\\\\cite{moustafa2021ton}',\n            'samples': len(y4),\n            'original_features': len(df4.columns),\n            'attack_examples': sorted(list(set(labels4.values())))[:5]\n        }\n    ]\n    \n    # Calculate combined statistics\n    total_samples = sum(ds['samples'] for ds in dataset_stats)\n    harmonized_features = len(all_columns)\n    \n    # Collect all unique attack types\n    all_attacks = set()\n    for labels_dict in [labels1, labels2, labels3, labels4]:\n        all_attacks.update(labels_dict.values())\n    \n    combined_attack_examples = sorted(list(all_attacks))[:6]\n    \n    # Generate LaTeX code\n    latex_code = \"\\\\documentclass[landscape]{article}\\n\"\n    latex_code += \"\\\\usepackage[margin=1in]{geometry}\\n\"\n    latex_code += \"\\\\usepackage{booktabs}\\n\"\n    latex_code += \"\\\\usepackage{multirow}\\n\"\n    latex_code += \"\\\\usepackage{array}\\n\"\n    latex_code += \"\\\\usepackage{makecell}\\n\\n\"\n    latex_code += \"\\\\begin{document}\\n\\n\"\n    latex_code += \"\\\\begin{table}[!htbp]\\n\"\n    latex_code += \"\\\\centering\\n\"\n    latex_code += \"\\\\caption{Comprehensive statistics of source datasets and fused corpus for IIoT intrusion detection.}\\n\"\n    latex_code += \"\\\\label{tab:dataset_statistics}\\n\"\n    latex_code += \"\\\\footnotesize\\n\"\n    latex_code += \"\\\\begin{tabular}{@{}llrrrl@{}}\\n\"\n    latex_code += \"\\\\toprule\\n\"\n    latex_code += \"\\\\textbf{Dataset Source} & \\\\textbf{Citation} & \\\\textbf{Total} & \\\\textbf{Original} & \\\\textbf{Harmonized} & \\\\textbf{Main Attack Classes} \\\\\\\\\\n\"\n    latex_code += \"& & \\\\textbf{Samples} & \\\\textbf{Features} & \\\\textbf{Features} & \\\\textbf{(Examples)} \\\\\\\\\\n\"\n    latex_code += \"\\\\midrule\\n\"\n    \n    # Add each dataset row\n    for i, ds in enumerate(dataset_stats):\n        attack_str = ', '.join(ds['attack_examples'][:3])\n        if len(ds['attack_examples']) > 3:\n            n_more = len(set(labels1.values()) if i == 0 else (set(labels2.values()) if i == 1 else (set(labels3.values()) if i == 2 else set(labels4.values())))) - 3\n            attack_str += f\", \\\\textit{{{{+{n_more} more}}}}\"\n        \n        latex_code += f\"{ds['name']} & {ds['citation']} & {ds['samples']:,} & {ds['original_features']} & {harmonized_features} & \\\\makecell[l]{{{{{attack_str}}}}} \\\\\\\\\\n\"\n    \n    # Add combined row\n    latex_code += \"\\\\midrule\\n\"\n    combined_attack_str = ', '.join(combined_attack_examples[:3])\n    if len(all_attacks) > 3:\n        combined_attack_str += f\", \\\\textit{{{{+{len(all_attacks) - 3} more}}}}\"\n    \n    latex_code += f\"\\\\textbf{{{{Fused Corpus}}}} & \\\\textbf{{{{---}}}} & \\\\textbf{{{{{total_samples:,}}}}} & \\\\textbf{{{{---}}}} & \\\\textbf{{{{{harmonized_features}}}}} & \\\\makecell[l]{{{{\\\\textbf{{{{{combined_attack_str}}}}}}}}} \\\\\\\\\\n\"\n    latex_code += \"\\\\bottomrule\\n\"\n    latex_code += \"\\\\end{tabular}\\n\"\n    latex_code += \"\\\\end{table}\\n\\n\"\n    latex_code += \"\\\\end{document}\\n\"\n    \n    # Save files\n    # output_file = Path(output_dir) / 'original_dataset_statistics_table.tex'\n    # with open(output_file, 'w', encoding='utf-8') as f:\n    #     f.write(latex_code)\n    \n    # logging.info(f\"LaTeX dataset statistics table generated: {output_file}\")\n    \n    # Save summary\n    # summary = {\n    #     'total_samples': total_samples,\n    #     'harmonized_features': harmonized_features,\n    #     'n_datasets': len(dataset_stats),\n    #     'n_unique_attacks': len(all_attacks),\n    #     'dataset_details': dataset_stats\n    # }\n    \n    # summary_file = Path(output_dir) / 'original_dataset_table_summary.json'\n    # with open(summary_file, 'w') as f:\n    #     json.dump(summary, f, indent=2)\n    \n    print(\"Dataset stats calculated (Text Only).\")\n    return latex_code\n\n\n# ---------------------------\n# Main function\n# ---------------------------\ndef main_unified():\n    # Output directory\n    output_dir = Path('/kaggle/working')\n    output_dir.mkdir(exist_ok=True)\n    \n    # ========================================\n    # 1) COLLECT ENVIRONMENT METADATA\n    # ========================================\n    logging.info(\"=\" * 60)\n    logging.info(\"COLLECTING ENVIRONMENT METADATA\")\n    logging.info(\"=\" * 60)\n    \n    SEED = 42\n    BATCH_SIZE = 32\n    \n    env_metadata = collect_environment_metadata(seed=SEED, batch_size=BATCH_SIZE)\n    \n    # with open(output_dir / 'original_env_meta.json', 'w') as f:\n    #     json.dump(env_metadata, f, indent=2)\n    \n    logging.info(f\"Environment metadata collected (Text Only)\")\n    \n    # ========================================\n    # DATA LOADING\n    # ========================================\n    PATH1 = '/kaggle/input/xiiotid-iiot-intrusion-dataset/X-IIoTID dataset.csv'\n    PATH2 = '/kaggle/input/edgeiiotset-cyber-security-dataset-of-iot-iiot/Edge-IIoTset dataset/Selected dataset for ML and DL/DNN-EdgeIIoT-dataset.csv'\n    PATH3 = '/kaggle/input/wustl-iiot-2021/wustl_iiot_2021.csv'\n    PATH4 = '/kaggle/input/ton-iot-train-test/TON_IoT_Train_Test_Network.csv'\n\n    # Data Preparation\n    df1, y1, le1, scaler1, labels1, class_dataset_map1 = prepare_data_1(PATH1)\n    ds1_labels = np.full(df1.shape[0], 1)\n    df2, y2, scaler2, classes2_arr, labels2, class_dataset_map2 = prepare_data_2(PATH2)\n    ds2_labels = np.full(df2.shape[0], 2)\n    df3, y3, le3, scaler3, labels3, class_dataset_map3 = prepare_data_3(PATH3)\n    ds3_labels = np.full(df3.shape[0], 3)\n    df4, y4, le4, scaler4, labels4, class_dataset_map4 = prepare_data_4(PATH4)\n    ds4_labels = np.full(df4.shape[0], 4)\n\n    # Label Harmonization\n    unique_combined_labels_names = np.unique(np.hstack((list(labels1.values()), list(labels2.values()),\n                                                         list(labels3.values()), list(labels4.values()))))\n    label_encoder_combined = LabelEncoder().fit(unique_combined_labels_names)\n    y1_encoded = np.array([label_encoder_combined.transform([labels1[val]])[0] for val in y1])\n    y2_encoded = np.array([label_encoder_combined.transform([labels2[val]])[0] for val in y2])\n    y3_encoded = np.array([label_encoder_combined.transform([labels3[val]])[0] for val in y3])\n    y4_encoded = np.array([label_encoder_combined.transform([labels4[val]])[0] for val in y4])\n    all_classes_encoded = np.unique(np.hstack((y1_encoded, y2_encoded, y3_encoded, y4_encoded)))\n    all_classes_labels_encoded_to_name = {i: label_encoder_combined.inverse_transform([i])[0] for i in all_classes_encoded}\n    class_dataset_mapping = {}\n    for d in [class_dataset_map1, class_dataset_map2, class_dataset_map3, class_dataset_map4]:\n        for label, ds_id in d.items():\n            if label not in class_dataset_mapping:\n                class_dataset_mapping[label] = ds_id\n    \n    # Feature Harmonization\n    logging.info(\"Harmonizing features using UNION followed by MEDIAN IMPUTATION...\")\n    all_columns = df1.columns.union(df2.columns).union(df3.columns).union(df4.columns)\n    logging.info(f\"Total unique features: {len(all_columns)}\")\n\n    df1_aligned = df1.reindex(columns=all_columns, fill_value=np.nan)\n    df2_aligned = df2.reindex(columns=all_columns, fill_value=np.nan)\n    df3_aligned = df3.reindex(columns=all_columns, fill_value=np.nan)\n    df4_aligned = df4.reindex(columns=all_columns, fill_value=np.nan)\n\n    X_combined_df = pd.concat([df1_aligned, df2_aligned, df3_aligned, df4_aligned], axis=0)\n    y_combined = np.hstack((y1_encoded, y2_encoded, y3_encoded, y4_encoded))\n    dataset_combined = np.hstack((ds1_labels, ds2_labels, ds3_labels, ds4_labels))\n\n    logging.info(\"Applying SimpleImputer with median strategy...\")\n    imputer = SimpleImputer(strategy='median')\n    X_combined = imputer.fit_transform(X_combined_df)\n    X_combined = np.nan_to_num(X_combined, nan=0.0, posinf=1e10, neginf=-1e10)\n\n    # ========================================\n    # 2) DATASET STATISTICS\n    # ========================================\n    logging.info(\"COLLECTING DATASET STATISTICS\")\n    \n    class_counts_dict = {}\n    for class_idx in np.unique(y_combined):\n        class_counts_dict[all_classes_labels_encoded_to_name[class_idx]] = int(np.sum(y_combined == class_idx))\n    \n    dataset_stats = {\n        'n_samples_total': int(len(X_combined)),\n        'n_features': int(X_combined.shape[1]),\n        'n_classes': int(len(np.unique(y_combined))),\n        'class_counts': class_counts_dict\n    }\n    \n    # with open(output_dir / 'original_dataset_stats.json', 'w') as f:\n    #     json.dump(dataset_stats, f, indent=2)\n\n    # Shuffle and Train/Test Split\n    X_combined, y_combined, dataset_combined = shuffle(X_combined, y_combined, dataset_combined, random_state=42)\n    X_train_combined, X_test_combined, y_train_combined, y_test_combined, dataset_train, dataset_test = train_test_split(\n        X_combined, y_combined, dataset_combined, test_size=0.2, random_state=42, stratify=y_combined\n    )\n\n    minority_classes_train_encoded = identify_minority_classes(y_train_combined)\n    minority_classes_train_names = [all_classes_labels_encoded_to_name[i] for i in minority_classes_train_encoded]\n    print(\"\\nMinority classes:\", minority_classes_train_names)\n\n    unique_classes_test = np.unique(y_test_combined)\n    target_names_map = {i: all_classes_labels_encoded_to_name[i] for i in unique_classes_test}\n    sorted_unique_classes = sorted(target_names_map.keys())\n    target_names = [f\"{target_names_map[i]}\" for i in sorted_unique_classes]\n    target_names_with_idx = [f\"{i} ({target_names_map[i]})\" for i in sorted_unique_classes]\n\n    # =================================================================================\n    # CREATE DISTRIBUTION PLOTS\n    # =================================================================================\n    logging.info(\"Creating distribution plots...\")\n    \n    fig_dist1 = plot_individual_dataset_distribution(y1, labels1, 1, 'Dataset 1 (X-IIoTID)', len(y1))\n    fig_dist2 = plot_individual_dataset_distribution(y2, labels2, 2, 'Dataset 2 (Edge-IIoTset)', len(y2))\n    fig_dist3 = plot_individual_dataset_distribution(y3, labels3, 3, 'Dataset 3 (WUSTL-IIOT-2021)', len(y3))\n    fig_dist4 = plot_individual_dataset_distribution(y4, labels4, 4, 'Dataset 4 (TON-IoT)', len(y4))\n    \n    combined_labels_map = {}\n    for encoded_val in y1_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    for encoded_val in y2_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    for encoded_val in y3_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    for encoded_val in y4_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    \n    y_list = [y1_encoded, y2_encoded, y3_encoded, y4_encoded]\n    labels_list = [combined_labels_map, combined_labels_map, combined_labels_map, combined_labels_map]\n    dataset_names = ['Dataset 1 (X-IIoTID)', 'Dataset 2 (Edge-IIoTset)', \n                    'Dataset 3 (WUSTL-IIOT-2021)', 'Dataset 4 (TON-IoT)']\n    fig_combined = plot_combined_dataset_distribution_stacked(y_list, labels_list, dataset_names)\n    \n    dataset_info = {\n        'Dataset 1 (X-IIoTID)': {'samples': len(y1), 'features': len(df1.columns), 'attacks': len(np.unique(y1))},\n        'Dataset 2 (Edge-IIoTset)': {'samples': len(y2), 'features': len(df2.columns), 'attacks': len(np.unique(y2))},\n        'Dataset 3 (WUSTL-IIOT-2021)': {'samples': len(y3), 'features': len(df3.columns), 'attacks': len(np.unique(y3))},\n        'Dataset 4 (TON-IoT)': {'samples': len(y4), 'features': len(df4.columns), 'attacks': len(np.unique(y4))},\n        'Combined': {'samples': len(X_combined), 'features': len(all_columns), 'attacks': len(np.unique(y_combined))}\n    }\n    fig_comparison = plot_dataset_comparison_horizontal(dataset_info)\n\n    fig_train_dist = plot_class_distribution(y_train_combined, all_classes_labels_encoded_to_name, \n                                           class_dataset_mapping, \"Class Distribution - Training Set\")\n    fig_test_dist = plot_class_distribution(y_test_combined, all_classes_labels_encoded_to_name, \n                                          class_dataset_mapping, \"Class Distribution - Test Set\")\n\n    # =================================================================================\n    # 3) MODEL TRAINING WITH RESOURCE MONITORING\n    # =================================================================================\n    logging.info(\"=\" * 60)\n    logging.info(\"STARTING MODEL TRAINING\")\n    logging.info(\"=\" * 60)\n    \n    # Start monitoring\n    start_monitoring()\n    \n    training_start_time = time.time()\n    model_results = {}\n    trained_models = {}\n    \n    # Model Training\n    models = {\n        'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0,\n                                 use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n        'Decision Tree': DecisionTreeClassifier(random_state=42),\n        'Random Forest': RandomForestClassifier(random_state=42),\n        'Extra Trees': ExtraTreesClassifier(random_state=42)\n    }\n\n    for name, model in models.items():\n        logging.info(f\"Training {name} model...\")\n        epoch_start = time.time()\n        \n        model.fit(X_train_combined, y_train_combined)\n        \n        epoch_time = time.time() - epoch_start\n        trained_models[name] = model\n        y_pred = model.predict(X_test_combined)\n        \n        # Calculate metrics\n        acc = accuracy_score(y_test_combined, y_pred)\n        prec = precision_score(y_test_combined, y_pred, average='weighted', zero_division=0)\n        rec = recall_score(y_test_combined, y_pred, average='weighted', zero_division=0)\n        f1 = f1_score(y_test_combined, y_pred, average='weighted', zero_division=0)\n        \n        model_results[name] = {\n            'accuracy': acc,\n            'precision': prec,\n            'recall': rec,\n            'f1_score': f1,\n            'train_time_s': epoch_time\n        }\n\n        print(f\"\\n--- {name} Evaluation ---\")\n        print(f\"Accuracy: {acc:.4f}\")\n        print(f\"Precision: {prec:.4f}\")\n        print(f\"Recall: {rec:.4f}\")\n        print(f\"F1-Score: {f1:.4f}\")\n        \n        report = classification_report(y_test_combined, y_pred, target_names=target_names_with_idx, zero_division=0)\n        print(colored_classification_report(report, class_dataset_mapping, all_classes_labels_encoded_to_name))\n        \n        gc.collect()\n\n    # VotingClassifier\n    voting_clf = VotingClassifier(estimators=[(name, model) for name, model in models.items()], voting='soft')\n    logging.info(\"Training VotingClassifier model...\")\n    ensemble_start = time.time()\n    \n    voting_clf.fit(X_train_combined, y_train_combined)\n    \n    ensemble_time = time.time() - ensemble_start\n    trained_models['VotingClassifier'] = voting_clf\n    y_pred_voting = voting_clf.predict(X_test_combined)\n    \n    acc_voting = accuracy_score(y_test_combined, y_pred_voting)\n    prec_voting = precision_score(y_test_combined, y_pred_voting, average='weighted', zero_division=0)\n    rec_voting = recall_score(y_test_combined, y_pred_voting, average='weighted', zero_division=0)\n    f1_voting = f1_score(y_test_combined, y_pred_voting, average='weighted', zero_division=0)\n    \n    model_results['VotingClassifier'] = {\n        'accuracy': acc_voting,\n        'precision': prec_voting,\n        'recall': rec_voting,\n        'f1_score': f1_voting,\n        'train_time_s': ensemble_time\n    }\n\n    print(f\"\\n--- VotingClassifier Evaluation ---\")\n    print(f\"Accuracy: {acc_voting:.4f}\")\n    print(f\"Precision: {prec_voting:.4f}\")\n    print(f\"Recall: {rec_voting:.4f}\")\n    print(f\"F1-Score: {f1_voting:.4f}\")\n    \n    report_voting = classification_report(y_test_combined, y_pred_voting, target_names=target_names_with_idx, zero_division=0)\n    print(colored_classification_report(report_voting, class_dataset_mapping, all_classes_labels_encoded_to_name))\n    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def main_unified():\n    # Output directory\n    output_dir = Path('../results')\n    output_dir.mkdir(exist_ok=True)\n    \n    # ========================================\n    # 1) COLLECT ENVIRONMENT METADATA\n    # ========================================\n    logging.info(\"=\" * 60)\n    logging.info(\"COLLECTING ENVIRONMENT METADATA\")\n    logging.info(\"=\" * 60)\n    \n    SEED = 42\n    BATCH_SIZE = 32\n    \n    env_metadata = collect_environment_metadata(seed=SEED, batch_size=BATCH_SIZE)\n    \n    # with open(output_dir / 'original_env_meta.json', 'w') as f:\n    #     json.dump(env_metadata, f, indent=2)\n    \n    logging.info(f\"Environment metadata collected (Text Only)\")\n    \n    # ========================================\n    # DATA LOADING\n    # ========================================\n    PATH1 = '../data/X-IIoTID dataset.csv'\n    PATH2 = '../data/DNN-EdgeIIoT-dataset.csv'\n    PATH3 = '../data/wustl_iiot_2021.csv'\n    PATH4 = '../data/TON_IoT_Train_Test_Network.csv'\n\n    # Data Preparation\n    df1, y1, le1, scaler1, labels1, class_dataset_map1 = prepare_data_1(PATH1)\n    ds1_labels = np.full(df1.shape[0], 1)\n    df2, y2, scaler2, classes2_arr, labels2, class_dataset_map2 = prepare_data_2(PATH2)\n    ds2_labels = np.full(df2.shape[0], 2)\n    df3, y3, le3, scaler3, labels3, class_dataset_map3 = prepare_data_3(PATH3)\n    ds3_labels = np.full(df3.shape[0], 3)\n    df4, y4, le4, scaler4, labels4, class_dataset_map4 = prepare_data_4(PATH4)\n    ds4_labels = np.full(df4.shape[0], 4)\n\n    # Label Harmonization\n    unique_combined_labels_names = np.unique(np.hstack((list(labels1.values()), list(labels2.values()),\n                                                         list(labels3.values()), list(labels4.values()))))\n    label_encoder_combined = LabelEncoder().fit(unique_combined_labels_names)\n    y1_encoded = np.array([label_encoder_combined.transform([labels1[val]])[0] for val in y1])\n    y2_encoded = np.array([label_encoder_combined.transform([labels2[val]])[0] for val in y2])\n    y3_encoded = np.array([label_encoder_combined.transform([labels3[val]])[0] for val in y3])\n    y4_encoded = np.array([label_encoder_combined.transform([labels4[val]])[0] for val in y4])\n    all_classes_encoded = np.unique(np.hstack((y1_encoded, y2_encoded, y3_encoded, y4_encoded)))\n    all_classes_labels_encoded_to_name = {i: label_encoder_combined.inverse_transform([i])[0] for i in all_classes_encoded}\n    class_dataset_mapping = {}\n    for d in [class_dataset_map1, class_dataset_map2, class_dataset_map3, class_dataset_map4]:\n        for label, ds_id in d.items():\n            if label not in class_dataset_mapping:\n                class_dataset_mapping[label] = ds_id\n    \n    # Feature Harmonization\n    logging.info(\"Harmonizing features using UNION followed by MEDIAN IMPUTATION...\")\n    all_columns = df1.columns.union(df2.columns).union(df3.columns).union(df4.columns)\n    logging.info(f\"Total unique features: {len(all_columns)}\")\n\n    df1_aligned = df1.reindex(columns=all_columns, fill_value=np.nan)\n    df2_aligned = df2.reindex(columns=all_columns, fill_value=np.nan)\n    df3_aligned = df3.reindex(columns=all_columns, fill_value=np.nan)\n    df4_aligned = df4.reindex(columns=all_columns, fill_value=np.nan)\n\n    X_combined_df = pd.concat([df1_aligned, df2_aligned, df3_aligned, df4_aligned], axis=0)\n    y_combined = np.hstack((y1_encoded, y2_encoded, y3_encoded, y4_encoded))\n    dataset_combined = np.hstack((ds1_labels, ds2_labels, ds3_labels, ds4_labels))\n\n    logging.info(\"Applying SimpleImputer with median strategy...\")\n    imputer = SimpleImputer(strategy='median')\n    X_combined = imputer.fit_transform(X_combined_df)\n    X_combined = np.nan_to_num(X_combined, nan=0.0, posinf=1e10, neginf=-1e10)\n\n    # ========================================\n    # 2) DATASET STATISTICS\n    # ========================================\n    logging.info(\"COLLECTING DATASET STATISTICS\")\n    \n    class_counts_dict = {}\n    for class_idx in np.unique(y_combined):\n        class_counts_dict[all_classes_labels_encoded_to_name[class_idx]] = int(np.sum(y_combined == class_idx))\n    \n    dataset_stats = {\n        'n_samples_total': int(len(X_combined)),\n        'n_features': int(X_combined.shape[1]),\n        'n_classes': int(len(np.unique(y_combined))),\n        'class_counts': class_counts_dict\n    }\n    \n    # with open(output_dir / 'original_dataset_stats.json', 'w') as f:\n    #     json.dump(dataset_stats, f, indent=2)\n\n    # Shuffle and Train/Test Split\n    X_combined, y_combined, dataset_combined = shuffle(X_combined, y_combined, dataset_combined, random_state=42)\n    X_train_combined, X_test_combined, y_train_combined, y_test_combined, dataset_train, dataset_test = train_test_split(\n        X_combined, y_combined, dataset_combined, test_size=0.2, random_state=42, stratify=y_combined\n    )\n\n    minority_classes_train_encoded = identify_minority_classes(y_train_combined)\n    minority_classes_train_names = [all_classes_labels_encoded_to_name[i] for i in minority_classes_train_encoded]\n    print(\"\\nMinority classes:\", minority_classes_train_names)\n\n    unique_classes_test = np.unique(y_test_combined)\n    target_names_map = {i: all_classes_labels_encoded_to_name[i] for i in unique_classes_test}\n    sorted_unique_classes = sorted(target_names_map.keys())\n    target_names = [f\"{target_names_map[i]}\" for i in sorted_unique_classes]\n    target_names_with_idx = [f\"{i} ({target_names_map[i]})\" for i in sorted_unique_classes]\n\n    # =================================================================================\n    # CREATE DISTRIBUTION PLOTS\n    # =================================================================================\n    logging.info(\"Creating distribution plots...\")\n    \n    fig_dist1 = plot_individual_dataset_distribution(y1, labels1, 1, 'Dataset 1 (X-IIoTID)', len(y1))\n    fig_dist2 = plot_individual_dataset_distribution(y2, labels2, 2, 'Dataset 2 (Edge-IIoTset)', len(y2))\n    fig_dist3 = plot_individual_dataset_distribution(y3, labels3, 3, 'Dataset 3 (WUSTL-IIOT-2021)', len(y3))\n    fig_dist4 = plot_individual_dataset_distribution(y4, labels4, 4, 'Dataset 4 (TON-IoT)', len(y4))\n    \n    combined_labels_map = {}\n    for encoded_val in y1_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    for encoded_val in y2_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    for encoded_val in y3_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    for encoded_val in y4_encoded:\n        combined_labels_map[encoded_val] = all_classes_labels_encoded_to_name[encoded_val]\n    \n    y_list = [y1_encoded, y2_encoded, y3_encoded, y4_encoded]\n    labels_list = [combined_labels_map, combined_labels_map, combined_labels_map, combined_labels_map]\n    dataset_names = ['Dataset 1 (X-IIoTID)', 'Dataset 2 (Edge-IIoTset)', \n                    'Dataset 3 (WUSTL-IIOT-2021)', 'Dataset 4 (TON-IoT)']\n    fig_combined = plot_combined_dataset_distribution_stacked(y_list, labels_list, dataset_names)\n    \n    dataset_info = {\n        'Dataset 1 (X-IIoTID)': {'samples': len(y1), 'features': len(df1.columns), 'attacks': len(np.unique(y1))},\n        'Dataset 2 (Edge-IIoTset)': {'samples': len(y2), 'features': len(df2.columns), 'attacks': len(np.unique(y2))},\n        'Dataset 3 (WUSTL-IIOT-2021)': {'samples': len(y3), 'features': len(df3.columns), 'attacks': len(np.unique(y3))},\n        'Dataset 4 (TON-IoT)': {'samples': len(y4), 'features': len(df4.columns), 'attacks': len(np.unique(y4))},\n        'Combined': {'samples': len(X_combined), 'features': len(all_columns), 'attacks': len(np.unique(y_combined))}\n    }\n    fig_comparison = plot_dataset_comparison_horizontal(dataset_info)\n\n    fig_train_dist = plot_class_distribution(y_train_combined, all_classes_labels_encoded_to_name, \n                                           class_dataset_mapping, \"Class Distribution - Training Set\")\n    fig_test_dist = plot_class_distribution(y_test_combined, all_classes_labels_encoded_to_name, \n                                          class_dataset_mapping, \"Class Distribution - Test Set\")\n\n    # =================================================================================\n    # 3) MODEL TRAINING WITH RESOURCE MONITORING\n    # =================================================================================\n    logging.info(\"=\" * 60)\n    logging.info(\"STARTING MODEL TRAINING\")\n    logging.info(\"=\" * 60)\n    \n    # Start monitoring\n    start_monitoring()\n    \n    training_start_time = time.time()\n    model_results = {}\n    trained_models = {}\n    \n    # Model Training\n    models = {\n        'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor', gpu_id=0,\n                                 use_label_encoder=False, eval_metric='mlogloss', random_state=42),\n        'Decision Tree': DecisionTreeClassifier(random_state=42),\n        'Random Forest': RandomForestClassifier(random_state=42),\n        'Extra Trees': ExtraTreesClassifier(random_state=42)\n    }\n\n    for name, model in models.items():\n        logging.info(f\"Training {name} model...\")\n        epoch_start = time.time()\n        \n        model.fit(X_train_combined, y_train_combined)\n        \n        epoch_time = time.time() - epoch_start\n        trained_models[name] = model\n        y_pred = model.predict(X_test_combined)\n        \n        # Calculate metrics\n        acc = accuracy_score(y_test_combined, y_pred)\n        prec = precision_score(y_test_combined, y_pred, average='weighted', zero_division=0)\n        rec = recall_score(y_test_combined, y_pred, average='weighted', zero_division=0)\n        f1 = f1_score(y_test_combined, y_pred, average='weighted', zero_division=0)\n        \n        model_results[name] = {\n            'accuracy': acc,\n            'precision': prec,\n            'recall': rec,\n            'f1_score': f1,\n            'train_time_s': epoch_time\n        }\n\n        print(f\"\\n--- {name} Evaluation ---\")\n        print(f\"Accuracy: {acc:.4f}\")\n        print(f\"Precision: {prec:.4f}\")\n        print(f\"Recall: {rec:.4f}\")\n        print(f\"F1-Score: {f1:.4f}\")\n        \n        report = classification_report(y_test_combined, y_pred, target_names=target_names_with_idx, zero_division=0)\n        print(colored_classification_report(report, class_dataset_mapping, all_classes_labels_encoded_to_name))\n        \n        gc.collect()\n\n    # VotingClassifier\n    voting_clf = VotingClassifier(estimators=[(name, model) for name, model in models.items()], voting='soft')\n    logging.info(\"Training VotingClassifier model...\")\n    ensemble_start = time.time()\n    \n    voting_clf.fit(X_train_combined, y_train_combined)\n    \n    ensemble_time = time.time() - ensemble_start\n    trained_models['VotingClassifier'] = voting_clf\n    y_pred_voting = voting_clf.predict(X_test_combined)\n    \n    acc_voting = accuracy_score(y_test_combined, y_pred_voting)\n    prec_voting = precision_score(y_test_combined, y_pred_voting, average='weighted', zero_division=0)\n    rec_voting = recall_score(y_test_combined, y_pred_voting, average='weighted', zero_division=0)\n    f1_voting = f1_score(y_test_combined, y_pred_voting, average='weighted', zero_division=0)\n    \n    model_results['VotingClassifier'] = {\n        'accuracy': acc_voting,\n        'precision': prec_voting,\n        'recall': rec_voting,\n        'f1_score': f1_voting,\n        'train_time_s': ensemble_time\n    }\n\n    print(f\"\\n--- VotingClassifier Evaluation ---\")\n    print(f\"Accuracy: {acc_voting:.4f}\")\n    print(f\"Precision: {prec_voting:.4f}\")\n    print(f\"Recall: {rec_voting:.4f}\")\n    print(f\"F1-Score: {f1_voting:.4f}\")\n    \n    report_voting = classification_report(y_test_combined, y_pred_voting, target_names=target_names_with_idx, zero_division=0)\n    print(colored_classification_report(report_voting, class_dataset_mapping, all_classes_labels_encoded_to_name))\n    \n    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n\n    total_training_time = time.time() - training_start_time\n    \n    # Stop monitoring\n    stop_monitoring()\n    \n    # ========================================\n    # 4) SAVE RESOURCE MONITORING DATA\n    # ========================================\n    logging.info(\"Saving resource monitoring data...\")\n    \n    resource_df = pd.DataFrame(resource_logs)\n    if len(resource_df) > 0:\n        resource_df['timestamp'] = pd.to_datetime(resource_df['timestamp'])\n        # resource_df.to_csv(output_dir / 'original_train_resource.csv', index=False)\n        \n        peak_ram_mb = float(resource_df['ram_used_mb'].max())\n        peak_gpu_mem_mb = float(resource_df['gpu_mem_used_mb'].max())\n        avg_cpu = float(resource_df['cpu_percent'].mean())\n    else:\n        peak_ram_mb = 0\n        peak_gpu_mem_mb = 0\n        avg_cpu = 0\n    \n    # ========================================\n    # 5) MODEL INFO\n    # ========================================\n    logging.info(\"COLLECTING MODEL INFORMATION\")\n    \n    # Save best model\n    import pickle\n    best_model = trained_models[best_model_name]\n    \n    # model_file = output_dir / 'original_best_model.pkl'\n    # with open(model_file, 'wb') as f:\n    #     pickle.dump(best_model, f)\n    \n    model_size_mb = 0 # model_file.stat().st_size / (1024**2)\n    \n    # Count parameters\n    n_parameters = 0\n    if hasattr(best_model, 'estimators_'):\n        for estimator in best_model.estimators_:\n            if hasattr(estimator, 'tree_'):\n                n_parameters += estimator.tree_.node_count\n            elif hasattr(estimator, 'estimators_'):\n                for tree in estimator.estimators_:\n                    if hasattr(tree, 'tree_'):\n                        n_parameters += tree.tree_.node_count\n    elif hasattr(best_model, 'get_booster'):\n        n_parameters = len(best_model.get_booster().get_dump())\n    \n    model_info = {\n        'model_name': best_model_name,\n        'model_size_mb': float(model_size_mb),\n        'n_parameters': int(n_parameters),\n        'architecture': str(type(best_model).__name__)\n    }\n    \n    # with open(output_dir / 'original_model_info.json', 'w') as f:\n    #     json.dump(model_info, f, indent=2)\n    \n    # ========================================\n    # 6) INFERENCE LATENCY MEASUREMENTS\n    # ========================================\n    logging.info(\"MEASURING INFERENCE LATENCY\")\n    \n    inference_sample = X_test_combined[:1000]\n    \n    latency_results = measure_inference_latency(\n        best_model, inference_sample, \n        n_runs=5, \n        batch_sizes=[1, 32], \n        device='GPU' if 'XGBoost' in best_model_name else 'CPU'\n    )\n    \n    latency_df = pd.DataFrame(latency_results)\n    # latency_df.to_csv(output_dir / 'original_inference_latency.csv', index=False)\n    \n    # ========================================\n    # 7) MINORITY CLASS EXPERIMENTS\n    # ========================================\n    logging.info(\"MINORITY CLASS EXPERIMENTS\")\n    \n    minority_results_all = []\n    \n    for minority_class in minority_classes_train_encoded:\n        if minority_class in unique_classes_test:\n            mask = (y_test_combined == minority_class)\n            if mask.sum() == 0:\n                continue\n            \n            y_true_class = y_test_combined[mask]\n            y_pred_class = y_pred_voting[mask]\n            \n            y_true_binary = (y_true_class == minority_class).astype(int)\n            y_pred_binary = (y_pred_class == minority_class).astype(int)\n            \n            recall = recall_score(y_true_binary, y_pred_binary, average='binary', zero_division=0)\n            precision = precision_score(y_true_binary, y_pred_binary, average='binary', zero_division=0)\n            f1 = f1_score(y_true_binary, y_pred_binary, average='binary', zero_division=0)\n            \n            minority_results_all.append({\n                'class': all_classes_labels_encoded_to_name[minority_class],\n                'recall': float(recall),\n                'precision': float(precision),\n                'f1_score': float(f1),\n                'support': int(mask.sum()),\n                'method': 'Baseline'\n            })\n    \n    baseline_macro_f1 = f1_score(y_test_combined, y_pred_voting, average='macro', zero_division=0)\n    \n    logging.info(f\"Baseline Macro-F1: {baseline_macro_f1:.4f}\")\n    \n    minority_df = pd.DataFrame(minority_results_all)\n    # minority_df.to_csv(output_dir / 'original_minority_results.csv', index=False)\n    \n    # ========================================\n    # 8) CROSS-DATASET EVALUATION\n    # ========================================\n    logging.info(\"CROSS-DATASET EVALUATION\")\n    \n    cross_domain_results = []\n    \n    macro_f1 = f1_score(y_test_combined, y_pred_voting, average='macro', zero_division=0)\n    micro_f1 = f1_score(y_test_combined, y_pred_voting, average='micro', zero_division=0)\n    recall_macro = recall_score(y_test_combined, y_pred_voting, average='macro', zero_division=0)\n    \n    cross_domain_results.append({\n        'train_dataset': 'Fused',\n        'test_dataset': 'Combined',\n        'macro_f1': float(macro_f1),\n        'micro_f1': float(micro_f1),\n        'recall_macro': float(recall_macro),\n        'train_time_s': float(total_training_time)\n    })\n    \n    cross_domain_df = pd.DataFrame(cross_domain_results)\n    # cross_domain_df.to_csv(output_dir / 'original_cross_domain_results.csv', index=False)\n    \n    # ========================================\n    # 9) IIOT SUITABILITY ANALYSIS\n    # ========================================\n    logging.info(\"ANALYZING IIOT IDS SUITABILITY\")\n    \n    iiot_suitability = {\n        'latency_analysis': {\n            'batch_1_mean_ms': float(latency_df[latency_df['batch_size'] == 1]['latency_mean_ms'].values[0]),\n            'batch_1_p95_ms': float(latency_df[latency_df['batch_size'] == 1]['latency_p95_ms'].values[0]),\n            'real_time_capable': bool(latency_df[latency_df['batch_size'] == 1]['latency_p95_ms'].values[0] < 100),\n            'throughput_samples_per_s': float(latency_df[latency_df['batch_size'] == 1]['throughput_samples_per_s'].values[0])\n        },\n        'resource_efficiency': {\n            'peak_ram_gb': float(peak_ram_mb / 1024),\n            'peak_gpu_mb': float(peak_gpu_mem_mb),\n            'avg_cpu_percent': float(avg_cpu),\n            'model_size_mb': float(model_size_mb),\n            'edge_deployable': bool(model_size_mb < 100 and peak_ram_mb < 2048),\n            'embedded_friendly': bool(model_size_mb < 50)\n        },\n        'detection_performance': {\n            'overall_accuracy': float(model_results[best_model_name]['accuracy']),\n            'macro_f1': float(baseline_macro_f1),\n            'micro_f1': float(micro_f1),\n            'minority_class_recall_avg': float(np.mean([r['recall'] for r in minority_results_all])) if minority_results_all else 0,\n            'balanced_performance': bool(baseline_macro_f1 > 0.7)\n        },\n        'operational_metrics': {\n            'training_time_hours': float(total_training_time / 3600),\n            'samples_processed': int(len(X_train_combined) + len(X_test_combined)),\n            'n_features': int(len(all_columns)),\n            'n_classes': int(len(unique_classes_test)),\n            'training_samples_per_second': float(len(X_train_combined) / total_training_time)\n        },\n        'iiot_readiness_score': 0.0\n    }\n    \n    # Calculate IIoT readiness score\n    score = 0\n    \n    if iiot_suitability['latency_analysis']['batch_1_p95_ms'] < 10:\n        score += 30\n    elif iiot_suitability['latency_analysis']['batch_1_p95_ms'] < 50:\n        score += 20\n    elif iiot_suitability['latency_analysis']['batch_1_p95_ms'] < 100:\n        score += 10\n    \n    if iiot_suitability['resource_efficiency']['edge_deployable']:\n        score += 15\n    if iiot_suitability['resource_efficiency']['embedded_friendly']:\n        score += 10\n    \n    acc_score = iiot_suitability['detection_performance']['overall_accuracy'] * 20\n    f1_score_val = baseline_macro_f1 * 15\n    score += acc_score + f1_score_val\n    \n    if iiot_suitability['detection_performance']['balanced_performance']:\n        score += 10\n    \n    iiot_suitability['iiot_readiness_score'] = float(min(100, score))\n    \n    if score >= 80:\n        suitability_level = \"EXCELLENT - Highly suitable for IIoT IDS deployment\"\n    elif score >= 60:\n        suitability_level = \"GOOD - Suitable for IIoT IDS with minor optimizations\"\n    elif score >= 40:\n        suitability_level = \"MODERATE - Suitable for cloud-based IIoT IDS\"\n    else:\n        suitability_level = \"LIMITED - Requires significant optimization\"\n    \n    iiot_suitability['suitability_assessment'] = suitability_level\n    \n    # with open(output_dir / 'original_iiot_suitability.json', 'w') as f:\n    #     json.dump(iiot_suitability, f, indent=2)\n    \n    # ========================================\n    # 10) GENERATE LATEX TABLES\n    # ========================================\n    logging.info(\"GENERATING LATEX TABLES\")\n    \n    # latex_output = output_dir / 'original_latex_tables.tex'\n    # \n    # with open(latex_output, 'w') as f:\n    #     f.write(\"% LaTeX Tables for IIoT IDS Manuscript\\n\\n\")\n    #     \n    #     # Table 1: Resource Footprint\n    #     f.write(\"\\\\begin{table}[ht]\\n\")\n    #     f.write(\"\\\\centering\\n\")\n    #     f.write(\"\\\\caption{Resource footprint and training cost for IIoT IDS model.}\\n\")\n    #     f.write(\"\\\\label{tab:original_resource_footprint}\\n\")\n    #     f.write(\"\\\\begin{tabular}{lrr}\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(\"Metric & Value & Unit \\\\\\\\\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(f\"Number of samples & {len(X_train_combined) + len(X_test_combined):,} & samples \\\\\\\\\\n\")\n    #     f.write(f\"Number of features & {len(all_columns)} &  \\\\\\\\\\n\")\n    #     f.write(f\"Number of classes & {len(unique_classes_test)} &  \\\\\\\\\\n\")\n    #     f.write(f\"Model file size & {model_size_mb:.2f} & MB \\\\\\\\\\n\")\n    #     f.write(f\"Peak RAM usage & {peak_ram_mb/1024:.2f} & GB \\\\\\\\\\n\")\n    #     f.write(f\"Training time & {total_training_time/3600:.2f} & h \\\\\\\\\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(\"\\\\end{tabular}\\n\")\n    #     f.write(\"\\\\end{table}\\n\\n\")\n    #     \n    #     # Table 2: Inference Latency\n    #     f.write(\"\\\\begin{table}[ht]\\n\")\n    #     f.write(\"\\\\centering\\n\")\n    #     f.write(\"\\\\caption{Inference latency and throughput for IIoT IDS deployment.}\\n\")\n    #     f.write(\"\\\\label{tab:original_inference_latency}\\n\")\n    #     f.write(\"\\\\begin{tabular}{lrrrr}\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(\"Batch size & Mean (ms) & P95 (ms) & Max (ms) & Throughput (samples/s) \\\\\\\\\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     \n    #     for _, row in latency_df.iterrows():\n    #         f.write(f\"{row['batch_size']} & {row['latency_mean_ms']:.2f} & {row['latency_p95_ms']:.2f} & \"\n    #                f\"{row['latency_max_ms']:.2f} & {row['throughput_samples_per_s']:.2f} \\\\\\\\\\n\")\n    #     \n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(\"\\\\end{tabular}\\n\")\n    #     f.write(\"\\\\end{table}\\n\\n\")\n    #     \n    #     # Table 3: Model Performance\n    #     f.write(\"\\\\begin{table}[ht]\\n\")\n    #     f.write(\"\\\\centering\\n\")\n    #     f.write(\"\\\\caption{Classification performance for IIoT attack detection.}\\n\")\n    #     f.write(\"\\\\label{tab:original_model_performance}\\n\")\n    #     f.write(\"\\\\begin{tabular}{lrrrr}\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(\"Model & Accuracy & Precision & Recall & F1-Score \\\\\\\\\\n\")\n    #     f.write(\"\\\\hline\\n\")\n    #     \n    #     for model_name, metrics in model_results.items():\n    #         f.write(f\"{model_name} & {metrics['accuracy']:.4f} & \"\n    #                f\"{metrics['precision']:.4f} & {metrics['recall']:.4f} & \"\n    #                f\"{metrics['f1_score']:.4f} \\\\\\\\\\n\")\n    #     \n    #     f.write(\"\\\\hline\\n\")\n    #     f.write(\"\\\\end{tabular}\\n\")\n    #     f.write(\"\\\\end{table}\\n\\n\")\n    \n    print(\"LaTeX tables generated (Text Only - Not Saved).\")\n    \n    # ========================================\n    # 11) GENERATE DATASET STATISTICS LATEX TABLE\n    # ========================================\n    logging.info(\"GENERATING DATASET STATISTICS LATEX TABLE\")\n    \n    generate_dataset_statistics_latex(\n        output_dir=output_dir,\n        df1=df1, df2=df2, df3=df3, df4=df4,\n        y1=y1, y2=y2, y3=y3, y4=y4,\n        labels1=labels1, labels2=labels2, labels3=labels3, labels4=labels4,\n        all_columns=all_columns\n    )\n    \n    # ========================================\n    # FINAL SUMMARY\n    # ========================================\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\ud83c\udfaf IIOT IDS ANALYSIS COMPLETE (TEXT ONLY)\")\n    print(\"=\" * 80)\n    print(f\"\\n\ud83d\udcca Key Results:\")\n    print(f\"   \u2022 Best Model: {best_model_name}\")\n    print(f\"   \u2022 Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n    print(f\"   \u2022 Macro-F1: {baseline_macro_f1:.4f}\")\n    print(f\"   \u2022 IIoT Readiness Score: {iiot_suitability['iiot_readiness_score']:.1f}/100\")\n    print(f\"\\n\u26a1 Performance Metrics:\")\n    print(f\"   \u2022 Inference Latency (P95): {iiot_suitability['latency_analysis']['batch_1_p95_ms']:.2f} ms\")\n    print(f\"   \u2022 Real-time Capable: {iiot_suitability['latency_analysis']['real_time_capable']}\")\n    print(f\"\\n\ud83d\udcc1 Generated Files: NONE (Text Only Mode)\")\n    print(\"\\n\" + \"=\" * 80)\n    print(\"\u2728 Ready for publication (Dry Run)!\")\n    print(\"=\" * 80 + \"\\n\")\n\nif __name__ == \"__main__\":\n    main_unified()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}